{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Objectif du notebook :\n",
    "int√©ragir avec Albert pour effectuer diff√©rentes op√©rations basiques : \n",
    "- ajouter/supprimer/lire collection\n",
    "- ajouter/supprimer/lire documents index√©s\n",
    "- poser des questions √† Albert\n",
    "- effectuer une recherche via /search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from openai import OpenAI\n",
    "import time\n",
    "import glob\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenAI client configuration\n",
    "base_url = \"https://albert.api.etalab.gouv.fr/v1\"\n",
    "api_key = os.getenv(\"ALBERT_API_KEY\")\n",
    "client = OpenAI(base_url=base_url, api_key=api_key)\n",
    "session = requests.session()\n",
    "session.headers = {\"Authorization\": f\"Bearer {api_key}\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download a file\n",
    "file_path = \"/home/pleroy/Downloads/anssi-guide-authentification_multifacteur_et_mots_de_passe.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "collection_id = \"ANSSI_test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "language_model, embeddings_model = None, None\n",
    "\n",
    "for model in client.models.list().data:\n",
    "    if model.type == \"text-generation\" and language_model is None:\n",
    "        language_model = model.id\n",
    "    if model.type == \"text-embeddings-inference\" and embeddings_model is None:\n",
    "        embeddings_model = model.id\n",
    "\n",
    "print(f\"language model: {language_model}\\nembeddings model: {embeddings_model}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "# Get Collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_collections():\n",
    "    response = session.get(f\"{base_url}/collections\")\n",
    "    response = response.json()\n",
    "    return response\n",
    "\n",
    "\n",
    "response = get_collections()\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "session.get(f\"{base_url}/collections\").json()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "# Delete colelction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "collections_id = [2137]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "for collection_id in collections_id:\n",
    "    print(collection_id)\n",
    "    response = session.delete(f\"{base_url}/collections/{collection_id}\")\n",
    "    print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "# Create collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "collection = \"ANSSI_test\"\n",
    "\n",
    "response = session.post(\n",
    "    f\"{base_url}/collections\", json={\"name\": collection, \"model\": embeddings_model}\n",
    ")\n",
    "response = response.json()\n",
    "collection_id = response[\"id\"]\n",
    "print(f\"Collection ID: {collection_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "# Get Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_collection(collection_id=collection_id):\n",
    "    response = session.get(f\"{base_url}/collections/{collection_id}\")\n",
    "    response = response.json()\n",
    "    return response\n",
    "\n",
    "\n",
    "get_collection(collection_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "# Lister documents index√©s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_documents(session, base_url, collection_id, limit=10, offset=0):\n",
    "    params = {\n",
    "        \"collection\": collection_id,\n",
    "        \"limit\": limit,\n",
    "        \"offset\": offset,\n",
    "    }\n",
    "    response = session.get(f\"{base_url}/documents\", params=params)\n",
    "    response.raise_for_status()\n",
    "    return response.json()\n",
    "\n",
    "\n",
    "def get_distinct_names(session, base_url, collection_id, limit=100, offset=0):\n",
    "    docs = get_documents(session, base_url, collection_id, limit, offset)\n",
    "    # Utilisation d‚Äôun set pour d√©dupliquer\n",
    "    names = {doc[\"name\"] for doc in docs.get(\"data\", []) if \"name\" in doc}\n",
    "    return sorted(names)\n",
    "\n",
    "\n",
    "# Exemple d‚Äôappel\n",
    "distinct_names = get_distinct_names(session, base_url, collection_id, limit=100)\n",
    "distinct_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "### Lister les documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def liste_dcouments_dans_collection(collection_id):\n",
    "    params = {\n",
    "        \"collection\": collection_id,\n",
    "        \"limit\": 100,\n",
    "        \"offset\": 0,\n",
    "    }\n",
    "    response = session.get(f\"{base_url}/documents\", params=params)\n",
    "    return response.json()\n",
    "\n",
    "\n",
    "liste_dcouments_dans_collection(collection_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "collection_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "### Lister les Chunks d'un document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "document_id = 963413\n",
    "response = session.get(f\"{base_url}/chunks/{document_id}\")\n",
    "response.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "# Ajouter pdf => /files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_file_deprecated(file_path, distinct_names):\n",
    "    files = {\n",
    "        \"file\": (os.path.basename(file_path), open(file_path, \"rb\"), \"application/pdf\")\n",
    "    }\n",
    "    data = {\"request\": '{\"collection\": \"%s\"}' % collection_id}\n",
    "\n",
    "    if Path(file_path).name not in distinct_names:\n",
    "        print(f\"File to add : {file_path}\")\n",
    "        response = session.post(f\"{base_url}/files\", data=data, files=files)\n",
    "        assert response.status_code == 201"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = glob.glob(\"/home/pleroy/Downloads/ANSSI/*.pdf\")\n",
    "for file_path in files:\n",
    "    add_file_deprecated(file_path, distinct_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "# Ajouter pdf => /documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_collection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Optional\n",
    "import json\n",
    "\n",
    "\n",
    "def ajouter_fichier(\n",
    "    chemin_fichier: str,\n",
    "    id_collection: str,\n",
    "    base_url: str,\n",
    "    url_source: Optional[str] = None,\n",
    ") -> requests.Response:\n",
    "    \"\"\"\n",
    "    T√©l√©verse un PDF dans Albert et ajoute l'URL d'origine en m√©tadonn√©e.\n",
    "\n",
    "    :param chemin_fichier: chemin local du PDF.\n",
    "    :param id_collection: identifiant de la collection cible.\n",
    "    :param url_source: URL publique du document.\n",
    "    :return: r√©ponse HTTP de l‚ÄôAPI.\n",
    "    \"\"\"\n",
    "    nom = Path(chemin_fichier).name\n",
    "    with open(chemin_fichier, \"rb\") as flux:\n",
    "        fichiers = {\"file\": (nom, flux, \"application/pdf\")}\n",
    "        donnees = {\n",
    "            \"collection\": str(id_collection),\n",
    "            \"metadata\": json.dumps({\"source_url\": url_source}),\n",
    "        }\n",
    "        # autres options possibles :\n",
    "        # \"paginate_output\": \"false\",\n",
    "        # \"force_ocr\": \"false\",\n",
    "        # \"output_format\": \"markdown\",\n",
    "        # \"chunk_size\": \"2048\",\n",
    "        reponse = session.post(f\"{base_url}/documents\", data=donnees, files=fichiers)\n",
    "    return reponse\n",
    "\n",
    "\n",
    "# Dictionnaire nom -> URL (faites correspondre √† vos fichiers locaux)\n",
    "URLS_PAR_NOM: Dict[str, str] = {\n",
    "    \"anssi-guide-authentification_multifacteur_et_mots_de_passe.pdf\": \"https://cyber.gouv.fr/sites/default/files/2021/10/anssi-guide-authentification_multifacteur_et_mots_de_passe.pdf\",\n",
    "    \"anssi-guide-gestion_crise_cyber.pdf\": \"https://cyber.gouv.fr/sites/default/files/2021/12/anssi-guide-gestion_crise_cyber.pdf\",\n",
    "    \"guide_hygiene_informatique_anssi.pdf\": \"https://cyber.gouv.fr/sites/default/files/2017/01/guide_hygiene_informatique_anssi.pdf\",\n",
    "    \"guide_nomadisme_anssi_pa_054_v2.pdf\": \"https://cyber.gouv.fr/sites/default/files/document/guide_nomadisme_anssi_pa_054_v2.pdf\",\n",
    "    \"anssi-guide-admin_securisee_si_v3-0.pdf\": \"https://cyber.gouv.fr/sites/default/files/2018/04/anssi-guide-admin_securisee_si_v3-0.pdf\",\n",
    "    \"anssi-guide-passerelle_internet_securisee-v3.pdf\": \"https://cyber.gouv.fr/sites/default/files/2020/06/anssi-guide-passerelle_internet_securisee-v3.pdf\",\n",
    "    \"anssi-fondamentaux-sauvegarde_systemes_dinformation_v1-0.pdf\": \"https://cyber.gouv.fr/sites/default/files/document/anssi-fondamentaux-sauvegarde_systemes_dinformation_v1-0.pdf\",\n",
    "    \"guide_protection_des_systemes_essentiels.pdf\": \"https://cyber.gouv.fr/sites/default/files/2020/12/guide_protection_des_systemes_essentiels.pdf\",\n",
    "    \"guide-homologation-securite-web-04-2025.pdf\": \"https://cyber.gouv.fr/sites/default/files/document/guide-homologation-securite-web-04-2025.pdf\",\n",
    "    \"anssi-guide-recommandations_mise_en_oeuvre_site_web_maitriser_standards_securite_cote_navigateur-v2.0.pdf\": \"https://cyber.gouv.fr/sites/default/files/2013/05/anssi-guide-recommandations_mise_en_oeuvre_site_web_maitriser_standards_securite_cote_navigateur-v2.0.pdf\",\n",
    "    \"secnumcloud-referentiel-exigences-v3.2.pdf\": \"https://cyber.gouv.fr/sites/default/files/document/secnumcloud-referentiel-exigences-v3.2.pdf\",\n",
    "    \"LAB_Homologation_Simplifiee.pdf\": \"https://monservicesecurise-ressources.cellar-c2.services.clever-cloud.com/LAB_Homologation_Simplifiee.pdf\",\n",
    "}\n",
    "\n",
    "\n",
    "def url_pour(chemin_fichier: str) -> Optional[str]:\n",
    "    \"\"\"Retourne l‚ÄôURL mapp√©e √† partir du nom de fichier, sinon None.\"\"\"\n",
    "    nom = Path(chemin_fichier).name\n",
    "    return URLS_PAR_NOM.get(nom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "fichiers = glob.glob(\"/home/pleroy/Downloads/ANSSI/*.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "fichiers = [fichiers[-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def est_deja_indexe(collection_id: str, nom_fichier: str) -> bool:\n",
    "    \"\"\"\n",
    "    V√©rifie si un document avec ce nom existe d√©j√† dans la collection.\n",
    "    \"\"\"\n",
    "    documents = liste_dcouments_dans_collection(collection_id)[\"data\"]\n",
    "    noms = {doc[\"name\"] for doc in documents}\n",
    "    return nom_fichier in noms\n",
    "\n",
    "\n",
    "nombre_documents_indexes = get_collection(collection_id)[\"documents\"]\n",
    "\n",
    "for chemin in fichiers:\n",
    "    nom_fichier = Path(chemin).name\n",
    "    # V√©rification pr√©alable\n",
    "    if est_deja_indexe(collection_id, nom_fichier):\n",
    "        print(f\"[SKIP] D√©j√† index√© : {nom_fichier}\")\n",
    "        continue\n",
    "\n",
    "    url = url_pour(chemin)\n",
    "    succes = False\n",
    "    tentative = 0\n",
    "\n",
    "    while tentative < 3 and not succes:\n",
    "        tentative += 1\n",
    "        print(f\"Tentative {tentative} pour {nom_fichier} ...\")\n",
    "\n",
    "        r = ajouter_fichier(\n",
    "            chemin,\n",
    "            collection_id,\n",
    "            base_url=base_url,\n",
    "            url_source=url,\n",
    "        )\n",
    "\n",
    "        nombre_documents_indexes_actuellement = get_collection(collection_id)[\n",
    "            \"documents\"\n",
    "        ]\n",
    "\n",
    "        if nombre_documents_indexes_actuellement - nombre_documents_indexes == 1:\n",
    "            print(f\"Le document a √©t√© index√© : {chemin}\")\n",
    "            succes = True\n",
    "            nombre_documents_indexes = nombre_documents_indexes_actuellement\n",
    "        else:\n",
    "            print(f\"Le document n'a pas √©t√© index√© : {chemin}\")\n",
    "            if tentative < 3:\n",
    "                print(\"Nouvel essai dans 5 secondes...\")\n",
    "                time.sleep(5)\n",
    "            else:\n",
    "                print(\"√âchec apr√®s 3 tentatives.\")\n",
    "\n",
    "        print(r.status_code)\n",
    "        print(\"*\" * 10)\n",
    "        try:\n",
    "            print(r.json())\n",
    "        except Exception:\n",
    "            print(f\"ERROR : {r.text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_collection(collection_id)[\"documents\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37",
   "metadata": {},
   "source": [
    "# Supprimer document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "session.delete(f\"{base_url}/documents/971024\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_collection()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40",
   "metadata": {},
   "source": [
    "# Poser questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41",
   "metadata": {},
   "source": [
    "### pour cette question: deux docs semblent dire la m√™me chose :\n",
    "- Quelles mesures doivent √™tre prises pour garantir la p√©rennit√© et la s√©curit√© d‚Äôune cartographie ?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import textwrap\n",
    "from tabulate import tabulate\n",
    "\n",
    "\n",
    "def ask_question(\n",
    "    question: str,\n",
    "    collection_id=None,\n",
    "    base_url=None,\n",
    "    session=None,\n",
    "    client=None,\n",
    "    k: int = 6,\n",
    "    show_table: str = \"itables\",  # \"itables\" | \"rich\" | \"tabulate\" | None\n",
    "):\n",
    "    \"\"\"\n",
    "    Pose une question (RAG + LLM) et affiche un tableau des chunks.\n",
    "    - show_table=\"itables\": DataFrame interactif avec itables (align√© √† gauche, chunk complet)\n",
    "      \"rich\": tableau terminal riche\n",
    "      \"tabulate\": tableau terminal compact\n",
    "      None: n'affiche pas le tableau\n",
    "    \"\"\"\n",
    "\n",
    "    # --- utilitaire pour versions \"rich/tabulate\" uniquement (aper√ßu court) ---\n",
    "    def _short(s: str, n: int) -> str:\n",
    "        s = re.sub(r\"\\s+\", \" \", (s or \"\").replace(\"\\n\", \" \").strip())\n",
    "        return textwrap.shorten(s, width=n, placeholder=\"‚Ä¶\")\n",
    "\n",
    "    # --- 1) Search ---\n",
    "    payload = {\n",
    "        \"collections\": [collection_id],\n",
    "        \"k\": k,\n",
    "        \"prompt\": question,\n",
    "        \"method\": \"semantic\",\n",
    "    }\n",
    "    resp = session.post(f\"{base_url}/search\", json=payload)\n",
    "    resp.raise_for_status()\n",
    "    search_data = resp.json().get(\"data\", [])\n",
    "    if not search_data:\n",
    "        raise RuntimeError(\n",
    "            \"Aucun chunk retourn√© par /search ‚Äî v√©rifie la collection/documents.\"\n",
    "        )\n",
    "\n",
    "    # --- 2) Pr√©parer les chunks + m√©ta ---\n",
    "    chunks_for_prompt = []\n",
    "    sources_set = set()\n",
    "\n",
    "    # Deux jeux de lignes :\n",
    "    # - rows_short : pour l'affichage terminal (aper√ßu tronqu√© lisible)\n",
    "    # - rows_full  : pour itables/pandas (chunk complet)\n",
    "    rows_short = []\n",
    "    rows_full = []\n",
    "\n",
    "    for idx, res in enumerate(search_data, start=1):\n",
    "        chunk = res.get(\"chunk\") or {}\n",
    "        meta = chunk.get(\"metadata\") or {}\n",
    "        content = chunk.get(\"content\", \"\")\n",
    "\n",
    "        doc_name = (\n",
    "            meta.get(\"document_name\") or meta.get(\"source\") or meta.get(\"file\") or \"‚Äî\"\n",
    "        )\n",
    "        page = meta.get(\"page\") or meta.get(\"page_number\")\n",
    "        score = res.get(\"score\")\n",
    "        source_url = meta.get(\"source_url\") or meta.get(\"url\") or meta.get(\"path\")\n",
    "\n",
    "        chunks_for_prompt.append(f\"[{idx}] {content}\")\n",
    "\n",
    "        rows_short.append(\n",
    "            {\n",
    "                \"#\": idx,\n",
    "                \"document\": _short(doc_name, 44),\n",
    "                \"page\": page if page is not None else \"\",\n",
    "                \"score\": f\"{score:.3f}\" if isinstance(score, (int, float)) else \"\",\n",
    "                \"source\": _short(source_url or \"\", 44),\n",
    "                \"aper√ßu\": _short(content, 120),\n",
    "            }\n",
    "        )\n",
    "\n",
    "        rows_full.append(\n",
    "            {\n",
    "                \"#\": idx,\n",
    "                \"document\": doc_name,\n",
    "                \"page\": page if page is not None else \"\",\n",
    "                \"score\": score,\n",
    "                \"source\": source_url or \"\",\n",
    "                \"chunk complet\": content,  # üî• texte int√©gral\n",
    "            }\n",
    "        )\n",
    "\n",
    "        if doc_name and doc_name != \"‚Äî\":\n",
    "            sources_set.add(doc_name)\n",
    "\n",
    "    # --- 3) Prompt template ---\n",
    "    prompt_template = \"\"\"\n",
    "Vous √™tes un assistant sp√©cialis√© dans la cybers√©curit√© et la conformit√©, qui doit r√©pondre uniquement √† partir des documents officiels fournis (issus de l‚ÄôANSSI).\n",
    "\n",
    "üéØ Objectif : Fournir une r√©ponse claire, pr√©cise, synth√©tique et factuelle √† la question de l‚Äôutilisateur, en vous basant exclusivement sur le contenu des documents donn√©s dans la section \"Documents\".\n",
    "\n",
    "‚ö†Ô∏è Contraintes :\n",
    "- N‚Äôinventez aucune information qui n‚Äôappara√Æt pas dans les documents fournis.\n",
    "- Si la r√©ponse ne figure pas dans les documents, r√©pondez simplement : \"Les documents fournis ne permettent pas de r√©pondre directement √† cette question.\"\n",
    "- Soyez concis, mais complet : privil√©giez la reformulation claire plut√¥t que la citation brute, sauf si une phrase exacte est n√©cessaire.\n",
    "- Int√©grez syst√©matiquement les r√©f√©rences aux documents (par leur nom ou source) pour justifier vos affirmations. Utilisez les indices [n] des morceaux comme renvois si utile.\n",
    "- Utilisez un style factuel, neutre et professionnel.\n",
    "\n",
    "Format attendu :\n",
    "- ‚úÖ Une r√©ponse directe et synth√©tique √† la question.\n",
    "\n",
    "Question :\n",
    "{user_question}\n",
    "\n",
    "Documents (morceaux) :\n",
    "{chunks_block}\n",
    "\"\"\".strip()\n",
    "\n",
    "    chunks_block = \"\\n\\n\".join(chunks_for_prompt)\n",
    "    final_prompt = prompt_template.format(\n",
    "        user_question=question, chunks_block=chunks_block\n",
    "    )\n",
    "\n",
    "    # --- 4) Appel mod√®le ---\n",
    "    completion = client.chat.completions.create(\n",
    "        messages=[{\"role\": \"user\", \"content\": final_prompt}],\n",
    "        model=\"albert-large\",\n",
    "        stream=False,\n",
    "        n=1,\n",
    "    )\n",
    "    answer = completion.choices[0].message.content\n",
    "\n",
    "    # --- 5) Affichages lisibles ---\n",
    "    print(\"\\n===== R√âPONSE =====\\n\")\n",
    "    print(textwrap.fill(answer, width=100))\n",
    "\n",
    "    print(\"\\n===== SOURCES (documents distincts) =====\")\n",
    "    if sources_set:\n",
    "        for s in sorted(sources_set):\n",
    "            print(\"-\", s)\n",
    "    else:\n",
    "        print(\"Aucune source disponible dans les m√©tadonn√©es.\")\n",
    "\n",
    "    # --- 6) Tableau des chunks ---\n",
    "    if show_table == \"itables\":\n",
    "        try:\n",
    "            from itables import show, options\n",
    "            import pandas as pd\n",
    "\n",
    "            # alignement √† gauche partout\n",
    "            options.columnDefs = [{\"targets\": \"_all\", \"className\": \"dt-left\"}]\n",
    "            df = pd.DataFrame(rows_full)\n",
    "            print(\"\\n===== CHUNKS (itables) =====\")\n",
    "            show(df, maxBytes=0)  # pas de troncature\n",
    "        except Exception as e:\n",
    "            print(f\"[itables indisponible] Fallback tabulate. D√©tail: {e}\")\n",
    "            show_table = \"tabulate\"  # bascule vers tabulate\n",
    "\n",
    "    if show_table == \"rich\":\n",
    "        try:\n",
    "            from rich.console import Console\n",
    "            from rich.table import Table\n",
    "\n",
    "            console = Console()\n",
    "            table = Table(show_header=True, header_style=\"bold\")\n",
    "            table.add_column(\"#\", justify=\"right\", width=3, no_wrap=True)\n",
    "            table.add_column(\"document\", overflow=\"ellipsis\", max_width=44)\n",
    "            table.add_column(\"page\", justify=\"right\", width=4, no_wrap=True)\n",
    "            table.add_column(\"score\", justify=\"right\", width=6, no_wrap=True)\n",
    "            table.add_column(\"source\", overflow=\"ellipsis\", max_width=44)\n",
    "            table.add_column(\"aper√ßu\", overflow=\"fold\", max_width=120)\n",
    "            for r in rows_short:\n",
    "                table.add_row(\n",
    "                    str(r[\"#\"]),\n",
    "                    r[\"document\"],\n",
    "                    str(r[\"page\"]),\n",
    "                    r[\"score\"],\n",
    "                    r[\"source\"],\n",
    "                    r[\"aper√ßu\"],\n",
    "                )\n",
    "            print(\"\\n===== CHUNKS (rich) =====\")\n",
    "            console.print(table)\n",
    "        except Exception as e:\n",
    "            print(f\"[rich indisponible] Fallback tabulate. D√©tail: {e}\")\n",
    "            show_table = \"tabulate\"\n",
    "\n",
    "    if show_table == \"tabulate\":\n",
    "        headers = [\"#\", \"document\", \"page\", \"score\", \"source\", \"aper√ßu\"]\n",
    "        print(\"\\n===== CHUNKS (tabulate) =====\")\n",
    "        print(\n",
    "            tabulate(\n",
    "                [[r[h] for h in headers] for r in rows_short],\n",
    "                headers=headers,\n",
    "                tablefmt=\"github\",\n",
    "                stralign=\"left\",\n",
    "                maxcolwidths=[3, 44, 4, 6, 44, 120],\n",
    "            )\n",
    "        )\n",
    "\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43",
   "metadata": {},
   "source": [
    "# questions avec r√©ponse satisfaisante :\n",
    "- Les mises √† jour logicielles sont-elles vraiment indispensables ?\n",
    "- Quels pare-feux dois-je d√©ployer pour s√©curiser l‚Äôinterconnexion entre mon SI et Internet ?\"\n",
    "- Quels pare-feux dois-je d√©ployer pour s√©curiser l‚Äôinterconnexion entre mon SI et Internet ?\"\n",
    "  Est-ce que je peux utiliser mon ordinateur personnel pour administrer le SI de mon organisation ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44",
   "metadata": {},
   "source": [
    "#¬†Questions sans r√©ponses :\n",
    "- Qui est responsable en cas de crise cyber ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45",
   "metadata": {},
   "source": [
    "reponse = ask_question(\"Les mises √† jour logicielles sont-elles vraiment indispensables ?\",\n",
    "                       collection_id=collection_id, \n",
    "                       base_url=base_url, \n",
    "                       session=session, \n",
    "                       client=client)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46",
   "metadata": {},
   "source": [
    "# Debug search\n",
    "objectif : voir les chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "\n",
    "def _search_semantic(collection_id, q, k=40):\n",
    "    payload = {\n",
    "        \"collections\": [collection_id],\n",
    "        \"k\": k,  # garde une valeur raisonnable (20‚Äì50)\n",
    "        \"method\": \"semantic\",\n",
    "        \"prompt\": q,\n",
    "    }\n",
    "    r = session.post(f\"{base_url}/search\", json=payload)\n",
    "    r.raise_for_status()\n",
    "    return r.json().get(\"data\", [])\n",
    "\n",
    "\n",
    "def distinct_document_names_semantic(collection_id, k=40, probes=None):\n",
    "    # Probes : termes vari√©s pour ‚Äúaccrocher‚Äù un max de chunks\n",
    "    if probes is None:\n",
    "        probes = [\n",
    "            \"cyber\",\n",
    "            \"s√©curit√©\",\n",
    "            \"incident\",\n",
    "            \"risque\",\n",
    "            \"sauvegarde\",\n",
    "            \"r√©ponse\",\n",
    "            \"attaque\",\n",
    "            \"ANSSI\",\n",
    "            \"gouvernance\",\n",
    "            \"politique\",\n",
    "        ]\n",
    "    names = set()\n",
    "    for q in probes:\n",
    "        for res in _search_semantic(collection_id, q, k=k):\n",
    "            meta = (res.get(\"chunk\", {}) or {}).get(\"metadata\", {}) or {}\n",
    "            name = meta.get(\"document_name\") or meta.get(\"source\") or meta.get(\"file\")\n",
    "            if name:\n",
    "                names.add(name)\n",
    "    return sorted(names)\n",
    "\n",
    "\n",
    "def document_name_counts_semantic(collection_id, k=40, probes=None):\n",
    "    if probes is None:\n",
    "        probes = [\n",
    "            \"cyber\",\n",
    "            \"s√©curit√©\",\n",
    "            \"incident\",\n",
    "            \"risque\",\n",
    "            \"sauvegarde\",\n",
    "            \"r√©ponse\",\n",
    "            \"attaque\",\n",
    "            \"ANSSI\",\n",
    "            \"gouvernance\",\n",
    "            \"politique\",\n",
    "        ]\n",
    "    counter = Counter()\n",
    "    for q in probes:\n",
    "        for res in _search_semantic(collection_id, q, k=k):\n",
    "            meta = (res.get(\"chunk\", {}) or {}).get(\"metadata\", {}) or {}\n",
    "            name = meta.get(\"document_name\") or meta.get(\"source\") or meta.get(\"file\")\n",
    "            if name:\n",
    "                counter[name] += 1\n",
    "    return counter\n",
    "\n",
    "\n",
    "# --- Utilisation ---\n",
    "docs = distinct_document_names_semantic(collection_id, k=40)\n",
    "print(\"Documents distincts :\", len(docs))\n",
    "for d in docs:\n",
    "    print(\"-\", d)\n",
    "\n",
    "print(\"\\nApprox. r√©partition des chunks vus :\")\n",
    "for name, cnt in document_name_counts_semantic(collection_id, k=40).most_common():\n",
    "    print(f\"{cnt:3d}  {name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48",
   "metadata": {},
   "source": [
    "# Recuperer les metaddata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Any, List, Optional\n",
    "import requests\n",
    "\n",
    "\n",
    "def liste_documents_avec_meta(\n",
    "    collection_id: int, base_url: str, session: requests.Session\n",
    ") -> List[Dict[str, Any]]:\n",
    "    # 1) tentative \"officielle\"\n",
    "    try:\n",
    "        r = session.get(f\"{base_url}/documents\", params={\"collection\": collection_id})\n",
    "        r.raise_for_status()\n",
    "        data = r.json().get(\"data\", [])\n",
    "        enriched: List[Dict[str, Any]] = []\n",
    "        for d in data:\n",
    "            if \"metadata\" in d and isinstance(d[\"metadata\"], dict):\n",
    "                enriched.append(d)\n",
    "            else:\n",
    "                # compl√®te par /documents/{id}\n",
    "                rd = session.get(f\"{base_url}/documents/{d['id']}\")\n",
    "                rd.raise_for_status()\n",
    "                jd = rd.json()\n",
    "                d[\"metadata\"] = jd.get(\"metadata\", {})\n",
    "                enriched.append(d)\n",
    "        return enriched\n",
    "    except requests.HTTPError as e:\n",
    "        # 404 => fallback search\n",
    "        if e.response is None or e.response.status_code != 404:\n",
    "            raise  # autre erreur -> remonter\n",
    "    return enriched\n",
    "\n",
    "\n",
    "liste_documents_avec_meta(collection_id, base_url, session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50",
   "metadata": {},
   "outputs": [],
   "source": [
    "collection_id"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
