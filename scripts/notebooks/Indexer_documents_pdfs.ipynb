{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Objectif du notebook :\n",
    "Indexer les documents pdfs pas encore indexés dans une collection existante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from openai import OpenAI\n",
    "import glob\n",
    "from typing import Dict, Optional\n",
    "import json\n",
    "import time\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "# Paramètres d'entrée"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "id_collection_existante = int(input(\"Insérez le numéro de la collection ID à laquelle vous souhaitez ajouter de \"))\n",
    "nom = input(\"Entrez le nom complet du fichier pdf à ajouter : \")\n",
    "url = input(\"Entrez l'URL du fichier à ajouter : \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = \"https://albert.api.etalab.gouv.fr/v1\"\n",
    "api_cle = os.getenv(\"ALBERT_API_KEY\")\n",
    "client = OpenAI(base_url=base_url, api_key=api_cle)\n",
    "session = requests.session()\n",
    "session.headers = {\"Authorization\": f\"Bearer {api_cle}\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "# Fonctions utilitaires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtient_documents(session, base_url, collection_id, limit=10, offset=0):\n",
    "    params = {\n",
    "        \"collection\": collection_id,\n",
    "        \"limit\": limit,\n",
    "        \"offset\": offset,\n",
    "    }\n",
    "    response = session.get(f\"{base_url}/documents\", params=params)\n",
    "    response.raise_for_status()\n",
    "    return response.json()\n",
    "\n",
    "\n",
    "def obtient_noms_uniques(session, base_url, collection_id, limit=100, offset=0):\n",
    "    docs = obtient_documents(session, base_url, collection_id, limit, offset)\n",
    "    names = {doc[\"name\"] for doc in docs.get(\"data\", []) if \"name\" in doc}\n",
    "    return sorted(names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "### Définition des Modèles LLMs utilisés"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_langue, model_embedding = None, None\n",
    "\n",
    "for model in client.models.list().data:\n",
    "    if model.type == \"text-generation\" and model_langue is None:\n",
    "        model_langue = model.id\n",
    "    if model.type == \"text-embeddings-inference\" and model_embedding is None:\n",
    "        model_embedding = model.id\n",
    "        \n",
    "print(f\"language model: {model_langue}\\nembeddings model: {model_embedding}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "# Ajouter des pdfs non indexés"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "fichiers = glob.glob(\"/home/pleroy/Downloads/ANSSI/*.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "#### Pour ajouter un nouveau document il faut ajouter le nom du pdf ainsi que l'url du pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionnaire nom -> URL (faites correspondre à vos fichiers locaux)\n",
    "URLS_PAR_NOM: Dict[str, str] = {\n",
    "    \"anssi-guide-authentification_multifacteur_et_mots_de_passe.pdf\": \"https://cyber.gouv.fr/sites/default/files/2021/10/anssi-guide-authentification_multifacteur_et_mots_de_passe.pdf\",\n",
    "    \"anssi-guide-gestion_crise_cyber.pdf\": \"https://cyber.gouv.fr/sites/default/files/2021/12/anssi-guide-gestion_crise_cyber.pdf\",\n",
    "    \"guide_hygiene_informatique_anssi.pdf\": \"https://cyber.gouv.fr/sites/default/files/2017/01/guide_hygiene_informatique_anssi.pdf\",\n",
    "    \"guide_nomadisme_anssi_pa_054_v2.pdf\": \"https://cyber.gouv.fr/sites/default/files/document/guide_nomadisme_anssi_pa_054_v2.pdf\",\n",
    "    \"anssi-guide-admin_securisee_si_v3-0.pdf\": \"https://cyber.gouv.fr/sites/default/files/2018/04/anssi-guide-admin_securisee_si_v3-0.pdf\",\n",
    "    \"anssi-guide-passerelle_internet_securisee-v3.pdf\": \"https://cyber.gouv.fr/sites/default/files/2020/06/anssi-guide-passerelle_internet_securisee-v3.pdf\",\n",
    "    \"anssi-fondamentaux-sauvegarde_systemes_dinformation_v1-0.pdf\": \"https://cyber.gouv.fr/sites/default/files/document/anssi-fondamentaux-sauvegarde_systemes_dinformation_v1-0.pdf\",\n",
    "    \"guide_protection_des_systemes_essentiels.pdf\": \"https://cyber.gouv.fr/sites/default/files/2020/12/guide_protection_des_systemes_essentiels.pdf\",\n",
    "    \"guide-homologation-securite-web-04-2025.pdf\": \"https://cyber.gouv.fr/sites/default/files/document/guide-homologation-securite-web-04-2025.pdf\",\n",
    "    \"anssi-guide-recommandations_mise_en_oeuvre_site_web_maitriser_standards_securite_cote_navigateur-v2.0.pdf\": \"https://cyber.gouv.fr/sites/default/files/2013/05/anssi-guide-recommandations_mise_en_oeuvre_site_web_maitriser_standards_securite_cote_navigateur-v2.0.pdf\",\n",
    "    \"secnumcloud-referentiel-exigences-v3.2.pdf\": \"https://cyber.gouv.fr/sites/default/files/document/secnumcloud-referentiel-exigences-v3.2.pdf\",\n",
    "    \"LAB_Homologation_Simplifiee.pdf\": \"https://monservicesecurise-ressources.cellar-c2.services.clever-cloud.com/LAB_Homologation_Simplifiee.pdf\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "URLS_PAR_NOM[nom] = url\n",
    "\n",
    "print(\"Ajouté :\", nom, \"->\", url)\n",
    "print(\"Dictionnaire mis à jour :\")\n",
    "for k, v in URLS_PAR_NOM.items():\n",
    "    print(f\"{k} -> {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def ajouter_fichier(\n",
    "    chemin_fichier: str,\n",
    "    id_collection: str,\n",
    "    base_url: str,\n",
    "    url_source: Optional[str] = None,\n",
    ") -> requests.Response:\n",
    "    \"\"\"\n",
    "    Téléverse un PDF dans Albert et ajoute l'URL d'origine en métadonnée.\n",
    "\n",
    "    :param chemin_fichier: chemin local du PDF.\n",
    "    :param id_collection: identifiant de la collection cible.\n",
    "    :param url_source: URL publique du document.\n",
    "    :return: réponse HTTP de l’API.\n",
    "    \"\"\"\n",
    "    nom = Path(chemin_fichier).name\n",
    "    with open(chemin_fichier, \"rb\") as flux:\n",
    "        fichiers = {\"file\": (nom, flux, \"application/pdf\")}\n",
    "        donnees = {\n",
    "            \"collection\": str(id_collection),\n",
    "            \"metadata\": json.dumps({\"source_url\": url_source}),\n",
    "        }\n",
    "        # autres options possibles :\n",
    "        # \"paginate_output\": \"false\",\n",
    "        # \"force_ocr\": \"false\",\n",
    "        # \"output_format\": \"markdown\",\n",
    "        # \"chunk_size\": \"2048\",\n",
    "        reponse = session.post(f\"{base_url}/documents\", data=donnees, files=fichiers)\n",
    "    return reponse\n",
    "\n",
    "    \n",
    "def url_pour(chemin_fichier: str) -> Optional[str]:\n",
    "    \"\"\"Retourne l’URL mappée à partir du nom de fichier, sinon None.\"\"\"\n",
    "    nom = Path(chemin_fichier).name\n",
    "    return URLS_PAR_NOM.get(nom)\n",
    "\n",
    "def est_deja_indexe(collection_id: str, nom_fichier: str) -> bool:\n",
    "    \"\"\"\n",
    "    Vérifie si un document avec ce nom existe déjà dans la collection.\n",
    "    \"\"\"\n",
    "    documents = liste_dcouments_dans_collection(collection_id)[\"data\"]\n",
    "    noms = {doc[\"name\"] for doc in documents}\n",
    "    return nom_fichier in noms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtient_collections(id_collection):\n",
    "    response = session.get(f\"{base_url}/collections/{id_collection}\")\n",
    "    response = response.json()\n",
    "    return response\n",
    "\n",
    "def liste_dcouments_dans_collection(id_collection):\n",
    "    params = {\n",
    "        \"collection\": id_collection,\n",
    "        \"limit\": 100,\n",
    "        \"offset\": 0,\n",
    "    }\n",
    "    response = session.get(f\"{base_url}/documents\", params=params)\n",
    "    return response.json()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "nombre_documents_indexes = obtient_collections(id_collection_existante)[\"documents\"]\n",
    "\n",
    "for chemin in fichiers:\n",
    "    nom_fichier = Path(chemin).name\n",
    "    # Vérification préalable\n",
    "    if est_deja_indexe(id_collection_existante, nom_fichier):\n",
    "        print(f\"[SKIP] Déjà indexé : {nom_fichier}\")\n",
    "        continue\n",
    "\n",
    "    url = url_pour(chemin)\n",
    "    succes = False\n",
    "    tentative = 0\n",
    "\n",
    "    while tentative < 3 and not succes:\n",
    "        tentative += 1\n",
    "        print(f\"Tentative {tentative} pour {nom_fichier} ...\")\n",
    "\n",
    "        r = ajouter_fichier(\n",
    "            chemin,\n",
    "            id_collection_existante,\n",
    "            base_url=base_url,\n",
    "            url_source=url,\n",
    "        )\n",
    "\n",
    "        nombre_documents_indexes_actuellement = obtient_collections(id_collection_existante)[\n",
    "            \"documents\"\n",
    "        ]\n",
    "\n",
    "        if nombre_documents_indexes_actuellement - nombre_documents_indexes == 1:\n",
    "            print(f\"Le document a été indexé : {chemin}\")\n",
    "            succes = True\n",
    "            nombre_documents_indexes = nombre_documents_indexes_actuellement\n",
    "        else:\n",
    "            print(f\"Le document n'a pas été indexé : {chemin}\")\n",
    "            if tentative < 3:\n",
    "                print(\"Nouvel essai dans 5 secondes...\")\n",
    "                time.sleep(5)\n",
    "            else:\n",
    "                print(\"Échec après 3 tentatives.\")\n",
    "\n",
    "        print(r.status_code)\n",
    "        print(\"*\" * 10)\n",
    "        try:\n",
    "            print(r.json())\n",
    "        except Exception:\n",
    "            print(f\"ERROR : {r.text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
